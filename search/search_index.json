{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"prefect-great-expectations","text":""},{"location":"#welcome","title":"Welcome!","text":"<p>Prefect integration for interacting with Great Expectations.</p> <p>Great Expectations is a Python library for data quality. It provides a framework to validate your state of data.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#python-setup","title":"Python setup","text":"<p>Requires an installation of Python 3.7+.</p> <p>We recommend using a Python virtual environment manager such as pipenv, conda, or virtualenv.</p> <p>These tasks are designed to work with Prefect 2. For more information about how to use Prefect, please refer to the Prefect documentation.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install <code>prefect-great-expectations</code> with <code>pip</code>:</p> <pre><code>pip install prefect-great-expectations\n</code></pre>"},{"location":"#write-and-run-a-flow","title":"Write and run a flow","text":"<pre><code>from prefect import flow\nfrom prefect_great_expectations import run_checkpoint_validation\n\n@flow\ndef example_flow():\n   run_checkpoint_validation(checkpoint_name=\"my_checkpoint\")\n\nexample_flow()\n</code></pre>"},{"location":"#tutorial","title":"Tutorial","text":"<p>For a larger example, check out the tutorial.</p>"},{"location":"#resources","title":"Resources","text":"<p>If you encounter any bugs while using <code>prefect-great-expectations</code>, feel free to open an issue in the prefect-great-expectations repository.</p> <p>If you have any questions or issues while using <code>prefect-great-expectations</code>, you can find help in either the Prefect Discourse forum or the Prefect Slack community.</p> <p>Feel free to \u2b50\ufe0f or watch <code>prefect-great-expectations</code> for updates too!</p>"},{"location":"#development","title":"Development","text":"<p>If you'd like to install a version of <code>prefect-great-expectations</code> for development, clone the repository and perform an editable install with <code>pip</code>:</p> <pre><code>git clone https://github.com/PrefectHQ/prefect-great-expectations.git\n\ncd prefect-great-expectations/\n\npip install -e \".[dev]\"\n\n# Install linting pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"tutorial/","title":"How to Use Great Expectations with Prefect","text":"<p>This guide will help you use Great Expectations with Prefect.</p> <p>Prefect is a workflow orchestration and observation platform that enables data engineers, ML engineers, and data scientists to stop wondering about their workflows. The Prefect open source library allows users to create workflows using Python and add retries, logging, caching, scheduling, failure notifications, and much more. Prefect Cloud offers all that goodness plus a hosted platform, automations, and enterprise features for users who need them. Prefect Cloud provides free and paid tiers.</p> <p>Prefect can be used with Great Expectations validations so that you can be confident about the state of your data. With a Prefect deployment, you can productionize your workflow and run data quality checks in reaction to the arrival of new data or on a schedule. </p>"},{"location":"tutorial/#doing-it","title":"Doing it","text":""},{"location":"tutorial/#install","title":"Install","text":"<p>Install the Great Expectations, Prefect, and prefect-great-expectations libraries into the same Python virtual environment. </p> <pre><code>pip install great_expectations prefect prefect_great_expectations\n</code></pre> <p>If you have any issues installing Prefect, check out the Prefect installation docs.</p>"},{"location":"tutorial/#create-an-expectation-suite-and-checkpoint","title":"Create an Expectation Suite and Checkpoint","text":"<p>Here's an example of a script to create an Expectation Suite and Checkpoint. This script is based on the Great Expectations Quickstart. </p> <pre><code>import great_expectations as gx\n\ndef create_expectation_suite_and_checkpoint():\n    \"\"\"Create a DataContext, connect to data, create Expectations, create and return a checkpoint.\"\"\"\n\n    context = gx.get_context()\n\n    validator = context.sources.pandas_default.read_csv(\n        \"https://raw.githubusercontent.com/great-expectations/gx_tutorials/main/data/yellow_tripdata_sample_2019-01.csv\"\n    )\n    validator.expect_column_values_to_not_be_null(\"pickup_datetime\")\n\n    # this expectation will fail\n    validator.expect_column_values_to_be_between(\n        \"passenger_count\", min_value=1, max_value=5\n    )\n\n    # checkpoints are reusble and only need to be created once\n    checkpoint = gx.checkpoint.SimpleCheckpoint(\n        name=\"taxi_check\",\n        data_context=context,\n        validator=validator,\n    )\n\n    return checkpoint\n</code></pre>"},{"location":"tutorial/#create-a-prefect-flow","title":"Create a Prefect flow","text":"<p>Like Great Expectations, Prefect is a Pythonic framework. In Prefect, you bring your Python code and sprinkle in task and flow decorators to gain observation and orchestration capabilities. </p> <p>Let's add a second function that we'll decorate with a Prefect <code>flow</code> decorator. Our flow function uses the <code>run_checkpoint_validation</code> task from the <code>prefect_great_expectations</code> library. This prebuilt function is a Prefect task that runs a Great Expectations validation. The <code>run_checkpoint_validation</code> can take a Great Expectations checkpoint as an argument. </p> <pre><code>from prefect import flow\nfrom prefect_great_expectations import run_checkpoint_validation\n\n@flow\ndef validation_flow(checkpoint):\n    \"\"\"Creates a task that validates a run of a Great Expectations checkpoint\"\"\"\n    res = run_checkpoint_validation(checkpoint=checkpoint)\n    return \n</code></pre> <p>Finally in our script, let's call our functions.</p> <pre><code>if __name__ == \"__main__\":\n    checkpoint = create_expectation_suite_and_checkpoint()\n    validation_flow(checkpoint=checkpoint)\n</code></pre> <p>Note that the second expectation will fail because the <code>passenger_count</code> column has some <code>6</code> values in the data. That's intentional so that we can see a failure example. Here's the output in our terminal window. </p> <pre><code>18:00:41.816 | INFO    | prefect.engine - Created flow run 'unyielding-husky' for flow 'validation-flow'\n18:00:43.847 | INFO    | Flow run 'unyielding-husky' - Created task run 'run_checkpoint_validation-0' for task 'run_checkpoint_validation'\n18:00:43.849 | INFO    | Flow run 'unyielding-husky' - Executing 'run_checkpoint_validation-0' immediately...\n18:00:44.786 | INFO    | Task run 'run_checkpoint_validation-0' - Running Great Expectations validation...\n...\n18:00:45.057 | ERROR   | Task run 'run_checkpoint_validation-0' - Encountered exception during execution:\n...\n    raise GreatExpectationValidationError(result)\nprefect_great_expectations.validation.GreatExpectationValidationError: Great Expectations Validation failed. Check result on this exception for more details.\n18:00:46.423 | ERROR   | Task run 'run_checkpoint_validation-0' - Finished in state Failed('Task run encountered an exception: prefect_great_expectations.validation.GreatExpectationValidationError: Great Expectations Validation failed. Check result on this exception for more details.\\n')\n18:00:46.424 | ERROR   | Flow run 'unyielding-husky' - Encountered exception during execution:\n18:00:46.916 | ERROR   | Flow run 'unyielding-husky' - Finished in state Failed('Flow run encountered an exception. prefect_great_expectations.validation.GreatExpectationValidationError: Great Expectations Validation failed...\n</code></pre>"},{"location":"tutorial/#avoid-raising-an-exception-on-validation-failure","title":"Avoid raising an exception on validation failure","text":"<p>If we want to avoid raising an exception when the validation fails, we can set the <code>raise_on_result</code> argument to <code>False</code> in the <code>run_checkpoint_validation</code> task. </p> <pre><code>@flow\ndef validation_flow(checkpoint):\n    \"\"\"Creates a task that validates a run of a Great Expectations checkpoint\"\"\"\n    res = run_checkpoint_validation(\n        checkpoint=checkpoint, raise_on_validation_failure=False\n    )\n    return\n</code></pre> <p>Now when we run our script we don't get an exception. </p> <pre><code>18:06:03.007 | INFO    | prefect.engine - Created flow run 'affable-malamute' for flow 'validation-flow'\n18:06:03.624 | INFO    | Flow run 'affable-malamute' - Created task run 'run_checkpoint_validation-0' for task 'run_checkpoint_validation'\n18:06:03.626 | INFO    | Flow run 'affable-malamute' - Executing 'run_checkpoint_validation-0' immediately...\n18:06:03.880 | INFO    | Task run 'run_checkpoint_validation-0' - Running Great Expectations validation...\n...\n18:06:04.138 | WARNING | Task run 'run_checkpoint_validation-0' - Great Expectations validation run  failed\n18:06:04.298 | INFO    | Task run 'run_checkpoint_validation-0' - Finished in state Completed()\n18:06:04.401 | INFO    | Flow run 'affable-malamute' - Finished in state Completed('All states completed.')\n</code></pre> <p>For more information about the <code>run_checkpoint_validation</code> task, refer to the prefect-great-expectations documentation.</p>"},{"location":"tutorial/#log-prints-for-more-information","title":"Log prints for more information","text":"<p>In the example above, we don't see all the relevant info for our validation failure. Let's print information about our validation results and log that information by passing <code>log_prints=True</code> to the <code>flow</code> decorator. </p> <pre><code>@flow(log_prints=True)\ndef validation_flow(checkpoint):\n    \"\"\"Creates a task that validates a run of a Great Expectations checkpoint\"\"\"\n    res = run_checkpoint_validation(\n        checkpoint=checkpoint, raise_on_validation_failure=False\n    )\n    print(res)\n    return\n</code></pre> <p>Now we can see lots of relevant information in our terminal window, including the following. </p> <pre><code>...\n \"partial_unexpected_counts\": [\n    {\n        \"value\": 6,\n        \"count\": 20\n    } \n...\n</code></pre> <p>Looks like we have 20 rows with a <code>6</code> in the <code>passenger_count</code> column.</p>"},{"location":"tutorial/#add-artifacts","title":"Add artifacts","text":"<p>If we fire up a locally hosted Prefect server or log in to our Prefect Cloud account, we can see the same information in the Prefect UI. In addtion, if we log in to Prefect Cloud we can create an artifact to share with our Prefect workspace collaborators. Let's do that now.</p> <ol> <li>Head over to https://app.prefect.cloud/ and sign up for a free account or log in to your existing account.</li> <li>Authenticate your command line client with <code>prefect cloud login</code>. </li> <li>Create an artifact to share your Great Expectations validation results with your collaborators. </li> </ol> <p>Prefect artifacts will persist the validation results from a flow run and display them in the UI. Let's create a Markdown artifact with the validation results.</p> <pre><code>from prefect.artifacts import create_markdown_artifact\n\n@flow(log_prints=True)\ndef validation_flow(checkpoint):\n    \"\"\"Creates a task that validates a run of a Great Expectations checkpoint\"\"\"\n    res = run_checkpoint_validation(\n        checkpoint=checkpoint, raise_on_validation_failure=False\n    )\n\n    create_markdown_artifact(\n        f\"\"\"# Result of Great Expectations validation run\n\n        {res}\n        \"\"\",\n        description=\"GX validation for Taxi Data\",\n        key=\"green-taxi-data\",\n    )\n\n    return\n</code></pre> <p>The UI gives you lots of visibilty into the state of your flow runs. </p> <p></p> <p>Your artifact displays validation results for human consumption.</p> <p></p> <p>Alternatively, you could share a link to your Great Expectations Data Docs in an artifact. </p>"},{"location":"tutorial/#wrap","title":"Wrap","text":"<p>You've seen how to use Prefect with Great Expectations. </p>"},{"location":"tutorial/#where-to-go-from-here","title":"Where to go from here","text":"<p>Prefect deployments allow you to run your flow in response to events such as the arrival of new data. You can also run on many types of schedules and on the infrastructure of your choice.</p> <p>There's lots more to explore for additional observability and orchestration with Prefect.</p> <p>Happy engineering!</p>"},{"location":"validation/","title":"Validation","text":"<p>Tasks for performing Great Expectations validations</p>"},{"location":"validation/#prefect_great_expectations.validation.GreatExpectationValidationError","title":"<code>GreatExpectationValidationError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Signals that a task failed due to a failed Great Expectations validation.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>CheckpointResult</code> <p>A CheckpointResult containing details of the failed validation.</p> required Source code in <code>prefect_great_expectations/validation.py</code> <pre><code>class GreatExpectationValidationError(Exception):\n    \"\"\"\n    Signals that a task failed due to a failed Great\n    Expectations validation.\n\n    Args:\n        result: A CheckpointResult containing details\n            of the failed validation.\n    \"\"\"\n\n    def __init__(self, result: CheckpointResult):\n        self.result = result\n        super().__init__(\n            \"Great Expectations Validation failed. \"\n            \"Check result on this exception for more details.\"\n        )\n</code></pre>"},{"location":"validation/#prefect_great_expectations.validation.run_checkpoint_validation","title":"<code>run_checkpoint_validation(run_name=None, checkpoint_name=None, checkpoint=None, checkpoint_kwargs=None, data_context_root_dir=None, data_context=None, runtime_environment=None, raise_on_validation_failure=True)</code>","text":"<p>Task that performs a Great Expectations validation based on the provided checkpoint     and data context.</p> <p>Parameters:</p> Name Type Description Default <code>run_name</code> <code>Optional[str]</code> <p>The name of the Great Expectations validation run. Defaults to timestamp if not provided.</p> <code>None</code> <code>checkpoint_name</code> <code>Optional[str]</code> <p>The name of the Checkpoint to use for validation.</p> <code>None</code> <code>checkpoint</code> <code>Optional[Checkpoint]</code> <p>A Checkpoint object to use for validation. Overrides <code>checkpoint_name</code> if both are provided.</p> <code>None</code> <code>checkpoint_kwargs</code> <code>Optional[Dict]</code> <p>A dictionary with values used to provide configuration to the task's Checkpint at runtime. Keys should match the parameters of <code>CheckpointConfig</code>.</p> <code>None</code> <code>data_context_root_dir</code> <code>Optional[Union[str, Path]]</code> <p>Path to the great_expectations directory.</p> <code>None</code> <code>data_context</code> <code>Optional[DataContext]</code> <p>A DataContext object to use during validation. Overrides <code>data_context_root_dir</code> if both are provided.</p> <code>None</code> <code>runtime_environment</code> <code>Optional[Dict]</code> <p>A dictionary with values to overwrite config in <code>great_expectations.yml</code> at run time.</p> <code>None</code> <code>raise_on_validation_failure</code> <code>bool</code> <p>If <code>True</code>, the task will raise a GreatExpectationValidationError when validation fails. If <code>False</code>, the task will return the result of the validation.</p> <code>True</code> <p>Raises:</p> Type Description <code>GreatExpectationValidationError</code> <p>Signals that a GE validation failed. Details of the failure can be found by inspecting the <code>result</code> attribute of the exception.</p> <p>Returns:</p> Name Type Description <code>CheckpointResult</code> <p>Detailed result of the validation run in the task.</p> <p>Examples:</p> <p>Run a validation with a checkpoint named 'my_checkpoint':</p> <pre><code>from prefect import flow\nfrom prefect_great_expectations import run_checkpoint_validation\n\n\n@flow\ndef example_flow():\n    run_checkpoint_validation(checkpoint_name=\"my_checkpoint\")\n\nexample_flow()\n</code></pre> <p>Run a validation with a custom path to the data context:</p> <pre><code>from prefect import flow\nfrom prefect_great_expectations import run_checkpoint_validation\n\n\n@flow\ndef example_flow():\n    run_checkpoint_validation(\n        checkpoint_name=\"my_checkpoint\",\n        data_context_root_dir=\"my_data_context/\"\n    )\n\nexample_flow()\n</code></pre> Source code in <code>prefect_great_expectations/validation.py</code> <pre><code>@task\ndef run_checkpoint_validation(\n    run_name: Optional[str] = None,\n    checkpoint_name: Optional[str] = None,\n    checkpoint: Optional[Checkpoint] = None,\n    checkpoint_kwargs: Optional[Dict] = None,\n    data_context_root_dir: Optional[Union[str, Path]] = None,\n    data_context: Optional[DataContext] = None,\n    runtime_environment: Optional[Dict] = None,\n    raise_on_validation_failure: bool = True,\n):\n    \"\"\"\n    Task that performs a Great Expectations validation based on the provided checkpoint\n        and data context.\n\n    Args:\n        run_name: The name of the Great Expectations validation run. Defaults to\n            timestamp if not provided.\n        checkpoint_name: The name of the Checkpoint to use for validation.\n        checkpoint: A Checkpoint object to use for validation. Overrides\n            `checkpoint_name` if both are provided.\n        checkpoint_kwargs: A dictionary with values used to provide configuration to\n            the task's Checkpint at runtime. Keys should match the parameters of\n            `CheckpointConfig`.\n        data_context_root_dir: Path to the great_expectations directory.\n        data_context: A DataContext object to use during validation. Overrides\n            `data_context_root_dir` if both are provided.\n        runtime_environment: A dictionary with values to overwrite config in\n            `great_expectations.yml` at run time.\n        raise_on_validation_failure: If `True`, the task will raise a\n            GreatExpectationValidationError when validation fails. If `False`,\n            the task will return the result of the validation.\n\n    Raises:\n        GreatExpectationValidationError: Signals that a GE validation failed.\n            Details of the failure can be found by inspecting the `result`\n            attribute of the exception.\n\n    Returns:\n        CheckpointResult: Detailed result of the validation run in the task.\n\n    Examples:\n        Run a validation with a checkpoint named 'my_checkpoint':\n\n        ```python\n        from prefect import flow\n        from prefect_great_expectations import run_checkpoint_validation\n\n\n        @flow\n        def example_flow():\n            run_checkpoint_validation(checkpoint_name=\"my_checkpoint\")\n\n        example_flow()\n        ```\n\n        Run a validation with a custom path to the data context:\n\n        ```python\n        from prefect import flow\n        from prefect_great_expectations import run_checkpoint_validation\n\n\n        @flow\n        def example_flow():\n            run_checkpoint_validation(\n                checkpoint_name=\"my_checkpoint\",\n                data_context_root_dir=\"my_data_context/\"\n            )\n\n        example_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n\n    logger.info(\"Running Great Expectations validation...\")\n\n    runtime_environment = runtime_environment or {}\n    checkpoint_kwargs = checkpoint_kwargs or {}\n\n    data_context_root_dir = (\n        str(data_context_root_dir) if data_context_root_dir else None\n    )\n\n    if data_context:\n        logger.debug(\"Using provided GE Data Context\")\n    else:\n        logger.debug(\"Loading GE Data Context from %s\", data_context_root_dir)\n        data_context = DataContext(\n            context_root_dir=data_context_root_dir,\n            runtime_environment=runtime_environment,\n        )\n\n    if checkpoint:\n        logger.debug(\"Using provided GE Checkpoint\")\n    else:\n        logger.debug(\"Loading GE Checkpoint with name %s\", checkpoint_name)\n        checkpoint = data_context.get_checkpoint(checkpoint_name)\n\n    result = checkpoint.run(run_name=run_name, **checkpoint_kwargs)\n\n    if not result.success:\n        logger.warning(\n            \"Great Expectations validation run %s failed\",\n            result.run_id.run_name if result.run_id.run_name else \"\",\n        )\n        if raise_on_validation_failure:\n            raise GreatExpectationValidationError(result)\n    else:\n        logger.info(\n            \"Great Expectations validation run %s succeeded\",\n            result.run_id.run_name if result.run_id.run_name else \"\",\n        )\n\n    return result\n</code></pre>"}]}